{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import h5py \n",
    "import os \n",
    "import math \n",
    "import numpy as np\n",
    "import struct as st\n",
    "from PIL import Image\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import makro_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = mu.load_MNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_probability(x_input, lts_size):\n",
    "    \n",
    "    mu = tf.layers.dense(inputs = x_input, units = lts_size, activation = 'relu', name = 'mpt_mu_layer_in')\n",
    "    sigma = tf.layers.dense(inputs = x_input, units = lts_size, activation = 'relu', name = 'mpt_sigma_layer_in')\n",
    "    \n",
    "    z = mu + sigma*tf.random.normal(tf.shape(mu), 0, 1, dtype = tf.float64)\n",
    "    \n",
    "    out = tf.layers.dense(inputs = z, units = x_input.get_shape().as_list()[1], activation = 'relu', name = 'mtp_out')\n",
    "    \n",
    "    return out, mu, sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batches_x_only(x_train, mini_batch_size):\n",
    "    \n",
    "    m = x_train.shape[0]\n",
    "    num_of_complete_mini_batches = math.floor(m/(mini_batch_size))\n",
    "    mini_batches = []\n",
    "    \n",
    "    for k in range(0, num_of_complete_mini_batches):\n",
    "        mini_batch_x = x_train[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :, :, :]\n",
    "        mini_batch = (mini_batch_x)\n",
    "        mini_batches.append(mini_batch)\n",
    "   \n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_x = x_train[num_of_complete_mini_batches * mini_batch_size : m, :, :, :]\n",
    "        mini_batch = (mini_batch_x)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff1(x_train, lts_size):\n",
    "    print(x_train.shape)\n",
    "    \n",
    "    layer_conv2d_1 = tf.layers.conv2d(inputs = x_train, filters = 10, kernel_size = (3,3), name = \"conv_2d_1\", activation = 'relu')\n",
    "    \n",
    "    layer_max_pool_1 = tf.layers.max_pooling2d(layer_conv2d_1, pool_size = (2,2) , strides = 2, padding='valid', name=\"max_pool_1\")\n",
    "    \n",
    "    layer_flatten = tf.layers.flatten(layer_max_pool_1)\n",
    "    \n",
    "    mtp, mu, sigma = map_to_probability(layer_flatten, lts_size)\n",
    "    \n",
    "    layer_expand = tf.reshape(mtp, tf.shape(layer_max_pool_1))\n",
    "    \n",
    "    layer_unpool_1 = tf.image.resize_images(layer_expand, tf.constant([int(layer_conv2d_1.shape[1]), int(layer_conv2d_1.shape[2])], tf.int32))\n",
    "   \n",
    "    layer_conv2d_1_decoder = tf.layers.conv2d(layer_unpool_1, filters = 1, kernel_size = (3, 3), strides = (1,1), padding=\"SAME\", activation=\"relu\")\n",
    "    \n",
    "    layer_unpool_2 = tf.image.resize_images(layer_conv2d_1_decoder, tf.constant([int(x_train.shape[1]), int(x_train.shape[2])], tf.int32))\n",
    "    \n",
    "    \n",
    "    print(x_train.shape)\n",
    "    print(layer_conv2d_1.shape)\n",
    "    print(layer_max_pool_1.shape)\n",
    "    print(layer_flatten.shape)\n",
    "    print(mtp.shape)\n",
    "    print(layer_expand.shape)\n",
    "    print(layer_unpool_1.shape)\n",
    "    print(layer_conv2d_1_decoder.shape)\n",
    "    print(layer_unpool_2.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return  layer_unpool_2, mu, sigma\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_train, lts_size = 20, learning_rate = 0.001, num_epochs = 100, minibatch_size = 10000, print_cost = True):\n",
    "    \n",
    "    x_train = np.expand_dims(x_train, axis=3)\n",
    "    ops.reset_default_graph() \n",
    "      \n",
    "    m = x_train.shape[0]  \n",
    "    costs = []                                                             \n",
    "    #Create placeholders\n",
    "    X = tf.placeholder(tf.float64, shape = (None ,x_train.shape[1], x_train.shape[2], 1))\n",
    "   \n",
    "    \n",
    "    \n",
    "    #Define loss function\n",
    "    y_predicted, mu, sigma = ff1(X, lts_size)\n",
    "    y_predicted = tf.cast(y_predicted, tf.float64)\n",
    "    reconstruction_loss = tf.reduce_sum(X - y_predicted, 1)\n",
    "    KL_divergence =  0.5*tf.reduce_sum(tf.square(mu) + tf.square(sigma) - tf.log(1e-8 + tf.square(sigma)) - 1, 1)  # Kullbackâ€“Leibler divergence\n",
    "    loss = (tf.reduce_mean(reconstruction_loss) - tf.reduce_mean(KL_divergence))\n",
    "    \n",
    "    #Initialize parameters and define optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Starting epoch: \", epoch)\n",
    "            \n",
    "            \n",
    "            \n",
    "            epoch_cost = 0.          \n",
    "            num_minibatches = int(m / minibatch_size) \n",
    "            minibatches = mini_batches_x_only(x_train, minibatch_size)\n",
    "            i = 0\n",
    "            for minibatch in minibatches:\n",
    "                i = 1 + i\n",
    "                print(i)\n",
    "            \n",
    "                minibatch_cost = sess.run(loss, feed_dict = {X: minibatch})\n",
    "                \n",
    "                epoch_cost = epoch_cost + minibatch_cost / num_minibatches\n",
    "                print(epoch_cost)\n",
    "                \n",
    "                if print_cost == True:\n",
    "                    costs.append(epoch_cost)\n",
    "                \n",
    "                \n",
    "                \n",
    "        #Plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
